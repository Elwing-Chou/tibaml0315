{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfDcCaTh0ijs3m31K2SyTg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elwing-Chou/tibaml0315/blob/main/imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0y2OyYDfe8G",
        "outputId": "6b75ea09-baaf-430d-839a-e03937d0d745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84125825/84125825 [==============================] - 5s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\n",
        "    fname=\"aclImdb.tar.gz\", \n",
        "    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "    extract=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "def getdata(mid):\n",
        "    dn = os.path.dirname(dataset)\n",
        "    posfn = glob.glob(os.path.join(dn, \"aclImdb\", mid, \"pos\", \"*\"))\n",
        "    negfn = glob.glob(os.path.join(dn, \"aclImdb\", mid, \"neg\", \"*\"))\n",
        "    contents = []\n",
        "    for fn in posfn + negfn:\n",
        "        with open(fn, encoding=\"utf-8\") as f:\n",
        "            contents.append(f.read())\n",
        "    df = pd.DataFrame({\n",
        "        \"content\":contents,\n",
        "        \"sentiment\":[1] * len(posfn) + [0] * len(negfn)\n",
        "    })\n",
        "    return df\n",
        "train_df = getdata(\"train\")\n",
        "test_df = getdata(\"test\")"
      ],
      "metadata": {
        "id": "tLi-Fe6zfl4M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "Q7E_-8mogCCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
        "# 3000種常用詞彙+1padding(0): 美一篇文章進入的時候只取512在常用詞彙列表的詞, 每一個詞化做100維度的語意像量\n",
        "layers = [\n",
        "    # 沒有激活, 3001(種詞彙) * 100 -> 300100\n",
        "    Embedding(input_dim=3001, output_dim=100, mask_zero=True, input_length=512),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(2, activation=\"softmax\")\n",
        "]\n",
        "model = Sequential(layers)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "jHXDnv-Uy2E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "model.compile(loss=SparseCategoricalCrossentropy(),\n",
        "       metrics=[\"accuracy\"],\n",
        "       optimizer=\"adam\")"
      ],
      "metadata": {
        "id": "QvptZClA5tA3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize: 詞彙換成數字, 建立一個3000常用詞彙辭典\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tok = Tokenizer(num_words=3000)\n",
        "tok.fit_on_texts(train_df[\"content\"])"
      ],
      "metadata": {
        "id": "bL8pdwvf5u_I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tok.word_index\n",
        "# tok.index_word\n",
        "# 檢查: 這個case, 標點和換行是可以去掉的\n",
        "# tok.word_index[\"?\"]\n",
        "# 停用詞(忽略一些無意義的): 不用, 根據答案就會把無意義的東西調整出來"
      ],
      "metadata": {
        "id": "WlFIWPpa6VDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Sequenize: 把我的字轉換成數字(利用剛剛列表)\n",
        "x_train_seq = tok.texts_to_sequences(train_df[\"content\"])\n",
        "x_test_seq = tok.texts_to_sequences(test_df[\"content\"])\n",
        "pd.DataFrame(x_train_seq)"
      ],
      "metadata": {
        "id": "O5Hn9oPP7seD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "x_train_pad = pad_sequences(x_train_seq, maxlen=512)\n",
        "x_test_pad = pad_sequences(x_test_seq, maxlen=512)\n",
        "pd.DataFrame(x_train_pad)"
      ],
      "metadata": {
        "id": "bRLGpZYW-cmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y_train = np.array(train_df[\"sentiment\"])\n",
        "y_test = np.array(test_df[\"sentiment\"])"
      ],
      "metadata": {
        "id": "s0AZB3dC_ulS"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}